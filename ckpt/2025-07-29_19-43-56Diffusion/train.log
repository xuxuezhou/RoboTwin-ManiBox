/home/xuxuezhou/miniconda3/envs/RoboTwin/lib/python3.10/site-packages/ultralytics/nn/tasks.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(file, map_location="cpu")
/home/xuxuezhou/code/RoboTwin/policy/ManiBox/manibox/ManiBox/dataloader/data_load.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(os.path.join(dataset_dir, "integration.pkl"), map_location='cpu')
/home/xuxuezhou/code/RoboTwin/policy/ManiBox/manibox/ManiBox/dataloader/BBoxHistoryEpisodicDataset.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data = torch.load(os.path.join(self.dataset_dir, "integration.pkl"), map_location='cpu')
/home/xuxuezhou/miniconda3/envs/RoboTwin/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/xuxuezhou/miniconda3/envs/RoboTwin/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Timestamp: 2025-07-29_19-43-56
scheduler: cos args.gradient_accumulation_steps 1
whether use acclerator: False
cur_path /home/xuxuezhou/code/RoboTwin/policy/ManiBox/manibox/ManiBox
num episodes 500
ðŸ”§ Using 2 object(s): ['apple', 'table']
ðŸ“Š Expected bbox dimension: 24

Data from: policy/ManiBox/processed_data/manibox-pick-diverse-bottles

Load data from policy/ManiBox/processed_data/manibox-pick-diverse-bottles/integration.pkl Shape:  torch.Size([500, 158, 1, 24])
image_data.shape, qpos_data.shape, action_data.shape:  torch.Size([90, 12]) torch.Size([90, 14]) torch.Size([90, 14])
Load data from policy/ManiBox/processed_data/manibox-pick-diverse-bottles/integration.pkl Shape:  torch.Size([500, 158, 1, 24])
image_data.shape, qpos_data.shape, action_data.shape:  torch.Size([90, 12]) torch.Size([90, 14]) torch.Size([90, 14])
length of train dataloader 4
You are using DiffusionPolicy.
policy_config {'lr': 0.002, 'lr_backbone': 7e-05, 'epochs': 50, 'train_loader_len': 4, 'warmup_ratio': 0.1, 'use_scheduler': 'cos', 'backbone': 'resnet18', 'masks': False, 'weight_decay': 0.0001, 'dilation': False, 'position_embedding': 'sine', 'loss_function': 'l1', 'chunk_size': 1, 'camera_names': ['cam_high', 'cam_left_wrist', 'cam_right_wrist'], 'num_next_action': 0, 'use_depth_image': False, 'use_robot_base': False, 'hidden_dim': 512, 'device': 'cuda:0', 'state_dim': 14, 'action_dim': 14, 'observation_horizon': 1, 'action_horizon': 8, 'num_inference_timesteps': 10, 'ema_power': 0.75, 'alpha': 3.0, 'max_time_steps': 1000, 'time_embed_dim': 128, 'context_len': 90, 'num_samples_per_traj': 10, 'policy_class': 'Diffusion', 'gradient_accumulation_steps': 1}
backbone visual encoder. number of parameters: 33.50M
diffusion model. number of parameters: 0.80M
  0%|          | 0/50 [00:00<?, ?it/s]
0it [00:00, ?it/s][A
                  [A
Epoch 0, lr: 0.0004
Train loss: 8.03617
loss: 8.036 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
  2%|â–         | 1/50 [00:01<00:49,  1.01s/it]Best ckpt saved, val loss 8.027401 @ epoch0
Val loss:   8.02740.   Best val loss: 8.02740 at epoch 0
loss: 8.027 

Epoch 1, lr: 0.0008
Train loss: 8.10040
loss: 8.100 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
  4%|â–         | 2/50 [00:01<00:35,  1.37it/s]Val loss:   8.20009.   Best val loss: 8.02740 at epoch 0
loss: 8.200 

Epoch 2, lr: 0.0012
Train loss: 8.06223
loss: 8.062 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
  6%|â–Œ         | 3/50 [00:02<00:30,  1.52it/s]Val loss:   8.09072.   Best val loss: 8.02740 at epoch 0
loss: 8.091 

Epoch 3, lr: 0.0016
Train loss: 7.91808
loss: 7.918 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
  8%|â–Š         | 4/50 [00:02<00:32,  1.41it/s]Best ckpt saved, val loss 7.801567 @ epoch3
Val loss:   7.80157.   Best val loss: 7.80157 at epoch 3
loss: 7.802 

Epoch 4, lr: 0.002
Train loss: 8.00216
loss: 8.002 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 10%|â–ˆ         | 5/50 [00:03<00:29,  1.52it/s]Val loss:   7.93111.   Best val loss: 7.80157 at epoch 3
loss: 7.931 

Epoch 5, lr: 0.0019975640502598244
Train loss: 7.91630
loss: 7.916 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 12%|â–ˆâ–        | 6/50 [00:04<00:31,  1.39it/s]Val loss:   7.82051.   Best val loss: 7.80157 at epoch 3
loss: 7.821 

Epoch 6, lr: 0.0019902680687415705
Train loss: 8.17150
loss: 8.172 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 14%|â–ˆâ–        | 7/50 [00:04<00:28,  1.50it/s]Val loss:   7.83977.   Best val loss: 7.80157 at epoch 3
loss: 7.840 

Epoch 7, lr: 0.0019781476007338056
Train loss: 7.89076
loss: 7.891 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 16%|â–ˆâ–Œ        | 8/50 [00:05<00:26,  1.56it/s]Val loss:   7.89740.   Best val loss: 7.80157 at epoch 3
loss: 7.897 

Epoch 8, lr: 0.001961261695938319
Train loss: 7.90607
loss: 7.906 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 18%|â–ˆâ–Š        | 9/50 [00:06<00:27,  1.47it/s]Best ckpt saved, val loss 7.637998 @ epoch8
Val loss:   7.63800.   Best val loss: 7.63800 at epoch 8
loss: 7.638 

Epoch 9, lr: 0.0019396926207859084
Train loss: 8.04242
loss: 8.042 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 20%|â–ˆâ–ˆ        | 10/50 [00:06<00:26,  1.52it/s]Val loss:   7.95151.   Best val loss: 7.63800 at epoch 8
loss: 7.952 

Epoch 10, lr: 0.001913545457642601
Train loss: 7.96271
loss: 7.963 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 22%|â–ˆâ–ˆâ–       | 11/50 [00:07<00:24,  1.59it/s]Val loss:   8.10363.   Best val loss: 7.63800 at epoch 8
loss: 8.104 

Epoch 11, lr: 0.001882947592858927
Train loss: 8.00867
loss: 8.009 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 24%|â–ˆâ–ˆâ–       | 12/50 [00:08<00:23,  1.59it/s]Val loss:   7.78796.   Best val loss: 7.63800 at epoch 8
loss: 7.788 

Epoch 12, lr: 0.0018480480961564258
Train loss: 8.02945
loss: 8.029 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:08<00:22,  1.66it/s]Val loss:   8.10280.   Best val loss: 7.63800 at epoch 8
loss: 8.103 

Epoch 13, lr: 0.0018090169943749475
Train loss: 8.02237
loss: 8.022 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:09<00:21,  1.67it/s]Val loss:   8.06801.   Best val loss: 7.63800 at epoch 8
loss: 8.068 

Epoch 14, lr: 0.001766044443118978
Train loss: 8.01695
loss: 8.017 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:09<00:21,  1.62it/s]Val loss:   8.13006.   Best val loss: 7.63800 at epoch 8
loss: 8.130 

Epoch 15, lr: 0.001719339800338651
Train loss: 8.00334
loss: 8.003 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:10<00:21,  1.61it/s]Val loss:   7.95360.   Best val loss: 7.63800 at epoch 8
loss: 7.954 

Epoch 16, lr: 0.0016691306063588583
Train loss: 8.04482
loss: 8.045 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:11<00:20,  1.63it/s]Val loss:   7.66424.   Best val loss: 7.63800 at epoch 8
loss: 7.664 

Epoch 17, lr: 0.0016156614753256582
Train loss: 8.09730
loss: 8.097 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:11<00:19,  1.65it/s]Val loss:   8.02082.   Best val loss: 7.63800 at epoch 8
loss: 8.021 

Epoch 18, lr: 0.0015591929034707468
Train loss: 8.01053
loss: 8.011 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:12<00:18,  1.67it/s]Val loss:   7.84634.   Best val loss: 7.63800 at epoch 8
loss: 7.846 

Epoch 19, lr: 0.0015
Train loss: 8.01983
loss: 8.020 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:12<00:17,  1.69it/s]Val loss:   7.91978.   Best val loss: 7.63800 at epoch 8
loss: 7.920 
/home/xuxuezhou/code/RoboTwin/policy/ManiBox/manibox/ManiBox/train.py:147: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure()

Epoch 20, lr: 0.0014383711467890773
Train loss: 7.96641
loss: 7.966 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:13<00:17,  1.65it/s]Val loss:   7.96820.   Best val loss: 7.63800 at epoch 8
loss: 7.968 

Epoch 21, lr: 0.0013746065934159121
Train loss: 7.88010
loss: 7.880 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:14<00:17,  1.63it/s]Val loss:   8.13496.   Best val loss: 7.63800 at epoch 8
loss: 8.135 

Epoch 22, lr: 0.0013090169943749475
Train loss: 8.02748
loss: 8.027 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:14<00:16,  1.61it/s]Val loss:   8.18145.   Best val loss: 7.63800 at epoch 8
loss: 8.181 

Epoch 23, lr: 0.0012419218955996676
Train loss: 7.90642
loss: 7.906 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:15<00:15,  1.64it/s]Val loss:   8.13121.   Best val loss: 7.63800 at epoch 8
loss: 8.131 

Epoch 24, lr: 0.0011736481776669307
Train loss: 8.01280
loss: 8.013 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:15<00:16,  1.55it/s]Val loss:   7.90842.   Best val loss: 7.63800 at epoch 8
loss: 7.908 

Epoch 25, lr: 0.0011045284632676536
Train loss: 8.07420
loss: 8.074 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:16<00:15,  1.57it/s]Val loss:   8.19521.   Best val loss: 7.63800 at epoch 8
loss: 8.195 

Epoch 26, lr: 0.0010348994967025011
Train loss: 7.91652
loss: 7.917 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:17<00:14,  1.56it/s]Val loss:   7.77172.   Best val loss: 7.63800 at epoch 8
loss: 7.772 

Epoch 27, lr: 0.0009651005032974994
Train loss: 7.97176
loss: 7.972 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:17<00:13,  1.57it/s]Val loss:   8.09718.   Best val loss: 7.63800 at epoch 8
loss: 8.097 

Epoch 28, lr: 0.0008954715367323467
Train loss: 8.04753
loss: 8.048 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:18<00:13,  1.56it/s]Val loss:   8.22159.   Best val loss: 7.63800 at epoch 8
loss: 8.222 

Epoch 29, lr: 0.0008263518223330697
Train loss: 7.92145
loss: 7.921 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:19<00:12,  1.57it/s]Val loss:   8.20555.   Best val loss: 7.63800 at epoch 8
loss: 8.206 

Epoch 30, lr: 0.0007580781044003324
Train loss: 7.99124
loss: 7.991 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:19<00:11,  1.58it/s]Val loss:   7.84997.   Best val loss: 7.63800 at epoch 8
loss: 7.850 

Epoch 31, lr: 0.0006909830056250527
Train loss: 7.99549
loss: 7.995 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:20<00:11,  1.61it/s]Val loss:   7.97762.   Best val loss: 7.63800 at epoch 8
loss: 7.978 

Epoch 32, lr: 0.0006253934065840879
Train loss: 7.94758
loss: 7.948 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:21<00:10,  1.57it/s]Val loss:   8.17154.   Best val loss: 7.63800 at epoch 8
loss: 8.172 

Epoch 33, lr: 0.0005616288532109225
Train loss: 7.98087
loss: 7.981 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:21<00:10,  1.59it/s]Val loss:   8.05613.   Best val loss: 7.63800 at epoch 8
loss: 8.056 

Epoch 34, lr: 0.0005000000000000002
Train loss: 7.94794
loss: 7.948 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:22<00:09,  1.57it/s]Val loss:   8.13642.   Best val loss: 7.63800 at epoch 8
loss: 8.136 

Epoch 35, lr: 0.0004408070965292533
Train loss: 8.00343
loss: 8.003 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:22<00:08,  1.56it/s]Val loss:   8.27928.   Best val loss: 7.63800 at epoch 8
loss: 8.279 

Epoch 36, lr: 0.0003843385246743417
Train loss: 8.07664
loss: 8.077 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:23<00:08,  1.55it/s]Val loss:   8.16361.   Best val loss: 7.63800 at epoch 8
loss: 8.164 

Epoch 37, lr: 0.0003308693936411421
Train loss: 8.04774
loss: 8.048 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:24<00:07,  1.56it/s]Val loss:   8.03094.   Best val loss: 7.63800 at epoch 8
loss: 8.031 

Epoch 38, lr: 0.00028066019966134904
Train loss: 7.95703
loss: 7.957 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [00:24<00:06,  1.58it/s]Val loss:   8.03384.   Best val loss: 7.63800 at epoch 8
loss: 8.034 

Epoch 39, lr: 0.0002339555568810221
Train loss: 8.02012
loss: 8.020 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [00:25<00:06,  1.55it/s]Val loss:   8.00422.   Best val loss: 7.63800 at epoch 8
loss: 8.004 

Epoch 40, lr: 0.00019098300562505265
Train loss: 7.98014
loss: 7.980 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [00:26<00:05,  1.54it/s]Val loss:   8.04107.   Best val loss: 7.63800 at epoch 8
loss: 8.041 

Epoch 41, lr: 0.00015195190384357404
Train loss: 8.02482
loss: 8.025 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [00:26<00:05,  1.53it/s]Val loss:   8.00969.   Best val loss: 7.63800 at epoch 8
loss: 8.010 

Epoch 42, lr: 0.00011705240714107302
Train loss: 8.08345
loss: 8.083 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [00:27<00:04,  1.51it/s]Val loss:   7.92723.   Best val loss: 7.63800 at epoch 8
loss: 7.927 

Epoch 43, lr: 8.645454235739902e-05
Train loss: 7.92525
loss: 7.925 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [00:28<00:03,  1.52it/s]Val loss:   8.17331.   Best val loss: 7.63800 at epoch 8
loss: 8.173 

Epoch 44, lr: 6.0307379214091684e-05
Train loss: 8.02710
loss: 8.027 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [00:28<00:03,  1.53it/s]Val loss:   7.92803.   Best val loss: 7.63800 at epoch 8
loss: 7.928 

Epoch 45, lr: 3.873830406168111e-05
Train loss: 7.92269
loss: 7.923 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:29<00:02,  1.52it/s]Val loss:   8.28281.   Best val loss: 7.63800 at epoch 8
loss: 8.283 

Epoch 46, lr: 2.1852399266194312e-05
Train loss: 8.04947
loss: 8.049 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [00:30<00:01,  1.50it/s]Val loss:   8.00844.   Best val loss: 7.63800 at epoch 8
loss: 8.008 

Epoch 47, lr: 9.731931258429638e-06
Train loss: 7.97892
loss: 7.979 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [00:31<00:01,  1.41it/s]Val loss:   8.00021.   Best val loss: 7.63800 at epoch 8
loss: 8.000 

Epoch 48, lr: 2.4359497401758024e-06
Train loss: 8.08181
loss: 8.082 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [00:31<00:00,  1.44it/s]Val loss:   8.16410.   Best val loss: 7.63800 at epoch 8
loss: 8.164 

Epoch 49, lr: 0.0
Train loss: 7.98200
loss: 7.982 
Saved plots to ./ckpt/2025-07-29_19-43-56Diffusion
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.53it/s]
