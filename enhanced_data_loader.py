#!/usr/bin/env python3
"""
å¢å¼ºçš„æ•°æ®è¯»å–è„šæœ¬
æ·»åŠ è¯¦ç»†çš„æ•°æ®æ‰“å°åŠŸèƒ½ï¼Œå°†æ‰€æœ‰æ•°æ®ä¿¡æ¯ä¿å­˜åˆ°logæ–‡ä»¶
"""

import os
import sys
import torch
import numpy as np
import json
import logging
from datetime import datetime
from collections import defaultdict

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
manibox_dir = os.path.join(current_dir, "policy/ManiBox/manibox/ManiBox")
if manibox_dir not in sys.path:
    sys.path.insert(0, manibox_dir)

from dataloader.BBoxHistoryEpisodicDataset import BBoxHistoryEpisodicDataset
from dataloader.data_load import get_norm_stats, load_data


def setup_logging(log_file):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def analyze_data_structure(data, logger):
    """åˆ†ææ•°æ®ç»“æ„"""
    logger.info("="*80)
    logger.info("ğŸ“Š æ•°æ®ç»“æ„åˆ†æ")
    logger.info("="*80)
    
    # åŸºæœ¬ä¿¡æ¯
    logger.info(f"æ•°æ®é”®: {list(data.keys())}")
    logger.info(f"æ•°æ®ç±»å‹: {type(data)}")
    
    # åˆ†ææ¯ä¸ªé”®çš„æ•°æ®
    for key, value in data.items():
        logger.info(f"\nğŸ” åˆ†æé”®: {key}")
        if isinstance(value, torch.Tensor):
            logger.info(f"   å½¢çŠ¶: {value.shape}")
            logger.info(f"   æ•°æ®ç±»å‹: {value.dtype}")
            logger.info(f"   è®¾å¤‡: {value.device}")
            logger.info(f"   æ•°å€¼èŒƒå›´: [{value.min():.6f}, {value.max():.6f}]")
            logger.info(f"   å‡å€¼: {value.mean():.6f}")
            logger.info(f"   æ ‡å‡†å·®: {value.std():.6f}")
            logger.info(f"   éé›¶å…ƒç´ æ•°é‡: {(value != 0).sum().item()}")
            logger.info(f"   æ€»å…ƒç´ æ•°é‡: {value.numel()}")
            logger.info(f"   éé›¶æ¯”ä¾‹: {(value != 0).float().mean():.2%}")
            
            # å¦‚æœæ˜¯å¤šç»´æ•°æ®ï¼Œåˆ†ææ¯ä¸ªç»´åº¦
            if len(value.shape) > 1:
                for dim in range(len(value.shape)):
                    dim_data = value.mean(dim=tuple(i for i in range(len(value.shape)) if i != dim))
                    logger.info(f"   ç»´åº¦{dim}ç»Ÿè®¡: èŒƒå›´=[{dim_data.min():.6f}, {dim_data.max():.6f}], å‡å€¼={dim_data.mean():.6f}")
        else:
            logger.info(f"   ç±»å‹: {type(value)}")
            logger.info(f"   å€¼: {value}")


def analyze_episode_data(data, episode_id, logger):
    """åˆ†æå•ä¸ªepisodeçš„æ•°æ®"""
    logger.info(f"\nğŸ¯ Episode {episode_id} è¯¦ç»†åˆ†æ")
    logger.info("-"*60)
    
    # è·å–episodeæ•°æ®
    if 'image_data' in data:
        image_data = data['image_data'][episode_id]
        logger.info(f"å›¾åƒæ•°æ®å½¢çŠ¶: {image_data.shape}")
        
        # åˆ†æbboxæ•°æ®
        if len(image_data.shape) >= 2:
            bbox_dim = image_data.shape[-1]
            logger.info(f"BBoxç»´åº¦: {bbox_dim}")
            
            # åˆ†ææ¯ä¸ªæ—¶é—´æ­¥çš„bbox
            for t in range(min(5, image_data.shape[0])):  # åªåˆ†æå‰5ä¸ªæ—¶é—´æ­¥
                bbox_data = image_data[t]
                logger.info(f"  æ—¶é—´æ­¥ {t}: bboxæ•°æ®èŒƒå›´=[{bbox_data.min():.6f}, {bbox_data.max():.6f}]")
                
                # è§£æbbox (å‡è®¾æ˜¯3ä¸ªç›¸æœºï¼Œæ¯ä¸ªç›¸æœº2ä¸ªæ£€æµ‹ï¼Œæ¯ä¸ªæ£€æµ‹4ä¸ªåæ ‡)
                if bbox_dim == 24:  # 3*2*4
                    for cam_idx, cam_name in enumerate(['head', 'left_wrist', 'right_wrist']):
                        for det_idx in range(2):
                            start_idx = cam_idx * 8 + det_idx * 4
                            bbox = bbox_data[start_idx:start_idx+4].tolist()
                            logger.info(f"    {cam_name}_cam, æ£€æµ‹{det_idx}: {bbox}")
    
    if 'qpos_data' in data:
        qpos_data = data['qpos_data'][episode_id]
        logger.info(f"å…³èŠ‚ä½ç½®æ•°æ®å½¢çŠ¶: {qpos_data.shape}")
        logger.info(f"å…³èŠ‚ä½ç½®èŒƒå›´: [{qpos_data.min():.6f}, {qpos_data.max():.6f}]")
        
        # åˆ†ææ¯ä¸ªå…³èŠ‚
        for joint_idx in range(min(5, qpos_data.shape[1])):  # åªåˆ†æå‰5ä¸ªå…³èŠ‚
            joint_data = qpos_data[:, joint_idx]
            logger.info(f"  å…³èŠ‚ {joint_idx}: èŒƒå›´=[{joint_data.min():.6f}, {joint_data.max():.6f}], å‡å€¼={joint_data.mean():.6f}")
    
    if 'action_data' in data:
        action_data = data['action_data'][episode_id]
        logger.info(f"åŠ¨ä½œæ•°æ®å½¢çŠ¶: {action_data.shape}")
        logger.info(f"åŠ¨ä½œèŒƒå›´: [{action_data.min():.6f}, {action_data.max():.6f}]")
        
        # åˆ†ææ¯ä¸ªåŠ¨ä½œç»´åº¦
        for action_idx in range(min(5, action_data.shape[1])):  # åªåˆ†æå‰5ä¸ªåŠ¨ä½œç»´åº¦
            action_dim_data = action_data[:, action_idx]
            logger.info(f"  åŠ¨ä½œç»´åº¦ {action_idx}: èŒƒå›´=[{action_dim_data.min():.6f}, {action_dim_data.max():.6f}], å‡å€¼={action_dim_data.mean():.6f}")


def analyze_dataset_statistics(data, logger):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    logger.info("\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
    logger.info("="*60)
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    stats = {}
    
    if 'image_data' in data:
        image_data = data['image_data']
        stats['image_data'] = {
            'shape': list(image_data.shape),
            'min': image_data.min().item(),
            'max': image_data.max().item(),
            'mean': image_data.mean().item(),
            'std': image_data.std().item(),
            'nonzero_ratio': (image_data != 0).float().mean().item()
        }
        logger.info(f"å›¾åƒæ•°æ®ç»Ÿè®¡: {stats['image_data']}")
    
    if 'qpos_data' in data:
        qpos_data = data['qpos_data']
        stats['qpos_data'] = {
            'shape': list(qpos_data.shape),
            'min': qpos_data.min().item(),
            'max': qpos_data.max().item(),
            'mean': qpos_data.mean().item(),
            'std': qpos_data.std().item()
        }
        logger.info(f"å…³èŠ‚ä½ç½®ç»Ÿè®¡: {stats['qpos_data']}")
    
    if 'action_data' in data:
        action_data = data['action_data']
        stats['action_data'] = {
            'shape': list(action_data.shape),
            'min': action_data.min().item(),
            'max': action_data.max().item(),
            'mean': action_data.mean().item(),
            'std': action_data.std().item()
        }
        logger.info(f"åŠ¨ä½œæ•°æ®ç»Ÿè®¡: {stats['action_data']}")
    
    return stats


def analyze_dataloader(dataloader, logger):
    """åˆ†ææ•°æ®åŠ è½½å™¨"""
    logger.info("\nğŸ”„ æ•°æ®åŠ è½½å™¨åˆ†æ")
    logger.info("="*60)
    
    logger.info(f"æ•°æ®åŠ è½½å™¨é•¿åº¦: {len(dataloader)}")
    
    # è·å–ç¬¬ä¸€ä¸ªbatchè¿›è¡Œåˆ†æ
    try:
        first_batch = next(iter(dataloader))
        logger.info(f"ç¬¬ä¸€ä¸ªbatchç±»å‹: {type(first_batch)}")
        
        if isinstance(first_batch, (list, tuple)):
            logger.info(f"ç¬¬ä¸€ä¸ªbatchåŒ…å« {len(first_batch)} ä¸ªå…ƒç´ ")
            for i, item in enumerate(first_batch):
                if isinstance(item, torch.Tensor):
                    logger.info(f"  å…ƒç´  {i}: å½¢çŠ¶={item.shape}, ç±»å‹={item.dtype}, èŒƒå›´=[{item.min():.6f}, {item.max():.6f}]")
                else:
                    logger.info(f"  å…ƒç´  {i}: ç±»å‹={type(item)}")
        elif isinstance(first_batch, torch.Tensor):
            logger.info(f"ç¬¬ä¸€ä¸ªbatch: å½¢çŠ¶={first_batch.shape}, ç±»å‹={first_batch.dtype}")
            logger.info(f"  èŒƒå›´: [{first_batch.min():.6f}, {first_batch.max():.6f}]")
            logger.info(f"  å‡å€¼: {first_batch.mean():.6f}")
            logger.info(f"  æ ‡å‡†å·®: {first_batch.std():.6f}")
    except Exception as e:
        logger.error(f"åˆ†ææ•°æ®åŠ è½½å™¨æ—¶å‡ºé”™: {e}")


def enhanced_load_data(dataset_dir, num_episodes, arm_delay_time, max_pos_lookahead, 
                      use_dataset_action, use_depth_image, use_robot_base, camera_names, 
                      batch_size_train, batch_size_val, episode_begin=0, episode_end=-1,
                      context_len=1, prefetch_factor=2, dataset_type=BBoxHistoryEpisodicDataset):
    """å¢å¼ºçš„æ•°æ®åŠ è½½å‡½æ•°ï¼ŒåŒ…å«è¯¦ç»†çš„æ•°æ®åˆ†æ"""
    
    # è®¾ç½®æ—¥å¿—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"data_analysis_{timestamp}.log"
    logger = setup_logging(log_file)
    
    logger.info("ğŸš€ å¼€å§‹å¢å¼ºæ•°æ®åŠ è½½åˆ†æ")
    logger.info(f"æ•°æ®é›†è·¯å¾„: {dataset_dir}")
    logger.info(f"Episodeæ•°é‡: {num_episodes}")
    logger.info(f"ç›¸æœºåç§°: {camera_names}")
    
    # åŠ è½½åŸå§‹æ•°æ®
    data_path = os.path.join(dataset_dir, "integration.pkl")
    logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
    
    if not os.path.exists(data_path):
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None, None, None, None
    
    try:
        data = torch.load(data_path, map_location='cpu')
        logger.info("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        
        # åˆ†ææ•°æ®ç»“æ„
        analyze_data_structure(data, logger)
        
        # åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = analyze_dataset_statistics(data, logger)
        
        # åˆ†æå‰å‡ ä¸ªepisodeçš„è¯¦ç»†æ•°æ®
        for episode_id in range(min(3, num_episodes)):
            analyze_episode_data(data, episode_id, logger)
        
        # è®¡ç®—æ ‡å‡†åŒ–ç»Ÿè®¡ä¿¡æ¯
        logger.info("\nğŸ“Š è®¡ç®—æ ‡å‡†åŒ–ç»Ÿè®¡ä¿¡æ¯")
        logger.info("="*60)
        
        norm_stats = get_norm_stats(dataset_dir, num_episodes, episode_begin, episode_end)
        logger.info(f"æ ‡å‡†åŒ–ç»Ÿè®¡ä¿¡æ¯: {norm_stats}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        logger.info("\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨")
        logger.info("="*60)
        
        train_dataloader, val_dataloader, norm_stats, is_sim = load_data(
            dataset_dir, num_episodes, arm_delay_time, max_pos_lookahead,
            use_dataset_action, use_depth_image, use_robot_base, camera_names,
            batch_size_train, batch_size_val, episode_begin, episode_end,
            context_len, prefetch_factor, dataset_type
        )
        
        # åˆ†ææ•°æ®åŠ è½½å™¨
        logger.info("\nğŸ“Š åˆ†æè®­ç»ƒæ•°æ®åŠ è½½å™¨")
        analyze_dataloader(train_dataloader, logger)
        
        logger.info("\nğŸ“Š åˆ†æéªŒè¯æ•°æ®åŠ è½½å™¨")
        analyze_dataloader(val_dataloader, logger)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSONæ–‡ä»¶
        stats_file = f"data_stats_{timestamp}.json"
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        norm_stats_serializable = {}
        for key, value in norm_stats.items():
            if isinstance(value, np.ndarray):
                norm_stats_serializable[key] = value.tolist()
            else:
                norm_stats_serializable[key] = value
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump({
                'dataset_info': {
                    'dataset_dir': dataset_dir,
                    'num_episodes': num_episodes,
                    'camera_names': camera_names,
                    'batch_size_train': batch_size_train,
                    'batch_size_val': batch_size_val
                },
                'data_statistics': stats,
                'normalization_stats': norm_stats_serializable,
                'is_simulation': is_sim
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        logger.info(f"âœ… è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        
        return train_dataloader, val_dataloader, norm_stats, is_sim
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None, None, None, None


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•å¢å¼ºæ•°æ®åŠ è½½"""
    # æµ‹è¯•å‚æ•°
    dataset_dir = "/home/xuxuezhou/code/RoboTwin/data/move_can_pot"
    num_episodes = 50
    camera_names = ['head_camera', 'left_camera', 'right_camera']
    batch_size_train = 32
    batch_size_val = 32
    
    print("ğŸš€ å¼€å§‹å¢å¼ºæ•°æ®åŠ è½½æµ‹è¯•")
    
    # è°ƒç”¨å¢å¼ºçš„æ•°æ®åŠ è½½å‡½æ•°
    train_dataloader, val_dataloader, norm_stats, is_sim = enhanced_load_data(
        dataset_dir=dataset_dir,
        num_episodes=num_episodes,
        arm_delay_time=0,
        max_pos_lookahead=0,
        use_dataset_action=True,
        use_depth_image=False,
        use_robot_base=False,
        camera_names=camera_names,
        batch_size_train=batch_size_train,
        batch_size_val=batch_size_val,
        episode_begin=0,
        episode_end=-1,
        context_len=1,
        prefetch_factor=2,
        dataset_type=BBoxHistoryEpisodicDataset
    )
    
    if train_dataloader is not None:
        print("âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"è®­ç»ƒæ•°æ®åŠ è½½å™¨é•¿åº¦: {len(train_dataloader)}")
        print(f"éªŒè¯æ•°æ®åŠ è½½å™¨é•¿åº¦: {len(val_dataloader)}")
    else:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥!")


if __name__ == "__main__":
    main() 